\section{Evaluation}
\label{s:eval}

\begin{table}[!t]
  \begin{scriptsize}
  \begin{tabular}{|p{0.13\textwidth}|p{0.18\textwidth}|p{0.04\textwidth}|p{0.04\textwidth}|}
    \hline
    Atom & Description & Area (\si{\micro\metre\squared}) at 1 GHz & Min. delay (picoseconds) \\
    \hline
    Stateless & Arithmetic, logic, relational, and conditional operations on packet/constant operands & 1384 & 387 \\
    \hline
    Read/Write & Read/Write packet field/constant into single state variable. & 250 & 176 \\
    \hline
    ReadAddWrite (RAW) & Add packet field/constant to state variable (OR) Write packet field/constant into state variable. & 431 & 316 \\
    \hline
    Predicated ReadAddWrite (PRAW) & Execute RAW on state variable only if a predicate is true, else leave unchanged. & 791 & 393 \\
    \hline
    IfElse ReadAddWrite (IfElseRAW) & Two separate RAWs: one each for when a predicate is true or false. & 985 & 392 \\
    \hline
    Subtract (Sub) & Same as IfElseRAW, but also allow subtracting a packet field/constant. & 1522 & 409 \\
    \hline
    Nested Ifs (Nested) & Same as Sub, but with an additional level of nesting that provides 4-way predication. & 3597 & 580 \\
    \hline
    Paired updates (Pairs) & Same as Nested, but allow updates to a pair of state variables, where predicates can use both state variables. & 5997 & 606 \\
    \hline
  \end{tabular}
  \end{scriptsize}
  \caption{Atom areas and minimum timing delays in a 32 nm standard-cell
  library.  All atoms meet timing at 1GHz. Each of the seven compiler targets
  contains one of the seven stateful atoms (Read/Write through Pairs) and the
  single stateless atom.}
  \label{tab:templates}
\end{table}

\begin{table*}[!t]
  \begin{tabular}{|p{0.16\textwidth}|p{0.34\textwidth}|p{0.08\textwidth}|p{0.09\textwidth}|p{0.07\textwidth}|p{0.06\textwidth}|p{0.05\textwidth}|}
\hline
Algorithm & Description & Least expressive atom & \# of stages, max. atoms/stage & Ingress or Egress Pipeline? & Domino LOC & P4 LOC\\
\hline
\pbox{0.16\textwidth}{Bloom filter\\(3 hash functions)} & \pbox{0.54\textwidth}{Set membership bit on every packet.} & Write & 4, 3 & Either & 29 & 104 \\
\hline
\pbox{0.16\textwidth}{Heavy Hitters~\cite{opensketch}\\(3 hash functions)} & Increment Count-Min Sketch~\cite{cormode} on every packet. & RAW & 10, 9 & Either & 35 & 192 \\
\hline
Flowlets~\cite{flowlets} & Update saved next hop if flowlet threshold is exceeded. & PRAW & 6, 2 & Ingress & 37 & 107 \\
\hline
RCP~\cite{rcp} & \pbox{0.34\textwidth}{Accumulate RTT sum if\\RTT is under maximum allowable RTT.} & PRAW & 3, 3 & Egress & 23 & 75 \\
\hline
\pbox{0.16\textwidth}{Sampled\\NetFlow~\cite{sampled_nflow}} & \pbox{0.47\textwidth}{Sample a packet if packet count reaches N;\\Reset count to 0 when it reaches N.} & IfElseRAW & 4, 2 & Either  & 18 & 70 \\
\hline
HULL~\cite{hull} & Update counter for virtual queue. & Sub & 7, 1 & Egress & 26 & 95 \\
\hline
\pbox{0.16\textwidth}{Adaptive\\Virtual Queue~\cite{avq}} & Update virtual queue size and virtual capacity & Nested & 7, 3 & Ingress & 36 & 147 \\
\hline
\pbox{0.16\textwidth}{Priority computation for weighted fair queueing~\cite{pifo_hotnets}} & Compute packet's virtual start time using finish time of last packet in that flow. & Nested & 4, 2 & Ingress & 29 & 87 \\
\hline
\pbox{0.16\textwidth}{DNS TTL change tracking~\cite{dns_change}} & Track number of changes in announced TTL for each domain & Nested & 6,3 & Ingress & 27 & 119 \\
\hline
CONGA~\cite{conga} & \pbox{0.34\textwidth}{Update best path's utilization/id if we see a better path.\\
                                           Update best path utilization alone if it changes.}  & Pairs & 4, 2 & Ingress & 32 & 89\\
\hline
%trTCM~\cite{trTCM} & Update token counts for each token bucket & Doesn't map & 7, 3 & Either \\
%\hline
CoDel~\cite{codel} & \pbox{0.34\textwidth}{Update:\\Whether we are marking or not.\\Time for next mark.\\Number of marks so far.\\Time at which min. queueing delay will exceed target.}& Doesn't map & 15, 3 & Egress & 57 & 271\\
\hline
\end{tabular}
\caption{Data-plane algorithms}
\label{tab:algos}
\end{table*}

We have performed a number of experiments to evaluate \pktlanguage. 
First, we evaluate \pktlanguage's expressiveness by using it to program several
data-plane algorithms (Table~\ref{tab:algos}), and comparing it to writing them
in P4~(\S\ref{ss:expressiveness}).  To validate that these algorithms can be
implemented at line rate, we design a concrete set of \absmachine machines that
we use as compiler targets for \pktlanguage~(\S\ref{ss:targets}).  We estimate
that these machines are feasible in hardware today because their atoms incur
modest chip area overhead. Next, we use the \pktlanguage compiler to compile
the algorithms in Table~\ref{tab:algos} to these targets~(\S\ref{ss:compiler}).
We conclude by quantifying the tradeoff between a target's programmability (the
space of data-plane algorithms that it can run at line rate) and the target's
performance (the maximum line rate it can support)~(\S\ref{ss:perfprog}).
%%
%%\footnote{We develop our own atoms since
%%no existing programmable switching chip supports a sufficiently rich set of
%%operations needed for data-plane algorithms.}

\subsection{Expressiveness}
\label{ss:expressiveness}

To evaluate \pktlanguage's expressiveness, we express several data-plane
algorithms (Table~\ref{tab:algos}) using \pktlanguage. These algorithms
encompass a variety of data-plane functionality including data-plane load
balancing, in-network congestion control, active queue management, security,
and measurement. In addition, we also used Domino to express the priority
computation for programming scheduling using the push-in first-out queue
abstraction~\cite{pifo_hotnets}. In all these cases, the algorithms are already
available as blocks of imperative code from online sources; translating them to
\pktlanguage syntax was straightforward.

In contrast, expressing any of these algorithms in P4 requires manually teasing
out portions of the algorithm that can reside in independent match-action
tables and then chaining these tables together. In essence, the programmer
manually carries out the transformations in \pktlanguage's compiler. Of the
algorithms in Table~\ref{tab:algos}, only flowlet switching has a publicly
available P4 implementation~\cite{p4_flowlet} that we can compare against. This
implementation requires 231 lines of uncommented P4, in comparison to the 37
lines of \pktlanguage code in Figure~\ref{fig:flowlet_code}. Not only that,
using P4 also requires the programmer to manually specify tables, the actions
within the tables, how tables are chained, and what headers are required---all
to implement a single data-plane algorithm. As the \pktlanguage compiler shows,
this process can be automated; to demonstrate this, we developed a backend for
\pktlanguage that generates the equivalent P4 code (lines of code for these
auto-generated P4 programs are listed in Table~\ref{tab:algos}).

Lastly, data-plane algorithms on software platforms today (NPUs,
Click~\cite{click}, the Linux qdisc subsystem~\cite{qdisc})  are programmed in
languages resembling \pktlanguage---hence we are confident that the
\pktlanguage syntax is already familiar to network operators.

\subsection{Compiler targets}
\label{ss:targets}

We design a concrete set of compiler targets for \pktlanguage based on the
\absmachine machine model (\S\ref{ss:absmachine}). First, we describe how to
assess the feasibility of atoms: whether they can run at a 1 GHz clock
frequency, and what area overhead they incur as a result. Next, we discuss the
design of stateless and stateful atoms separately. Finally, we discuss how
these stateless and stateful atoms are combined together in our compiler
targets.

\paragraph{Atom feasibility}
For both stateless and stateful atoms, we first specify atoms using atom
templates. We then synthesize a digital circuit corresponding to each atom
template by writing the atom as a module in Verilog, and using the Synopsys
Design Compiler~\cite{synopsys_dc} to check if it meets timing at 1 GHz in a
32-nm standard-cell library (a library of primitive gates designed using a
transistor with a feature size of 32 nm).  Second, we use an individual atom's
gate area, also obtained from the Synopsys Design Compiler, and a switching
chip's area~\cite{gibb_parsing}, to estimate the area overhead for provisioning
a \absmachine machine with multiple instances of each atom.

\paragraph{Designing stateless atoms}
Stateless atoms are easier to design because arbitrary stateless operations can
be spread out across multiple pipeline stages without violating
atomicity~(\S\ref{ss:atoms}). We design a stateless atom that can support
simple arithmetic (add, subtract, left shift, right shift), logical (and, or,
xor), relational ({\tt >=}, {\tt <=}, {\tt ==}, {\tt !=}), or conditional
operations (C's ``{\tt ?}'' operator) on a set of packet fields. Any packet
field can also be substituted with a constant operand.

\paragraph{Designing stateful atoms}
Designing stateful atoms is more involved because the choice of stateful atoms
determines which algorithms a line-rate switch can support. A more complex
stateful atom can support more data-plane algorithms, but has a higher chance
of not meeting timing, and occupies greater chip area. To illustrate this
effect, we design a containment hierarchy of stateful atoms, where each atom
can express all stateful operations that its predecessor can. Concretely, these
atoms start out with the simplest stateful capability: the ability to read or
write state alone.  They then move on to the ability to read, add, and write
back state atomically (RAW), a predicated version of the same (PRAW), and so
on. When synthesized to a 32 nm standard-cell library, all of our designed
atoms meet timing at 1 GHz. However, the atom's area and minimum end-to-end
propagation delay increases with the atom's
complexity~(Table~\ref{tab:templates}).

\paragraph{The compiler targets}
We design seven compiler targets: each target combines one of the seven
stateful atoms in Table~\ref{tab:templates} with the single stateless atom in
the same table. For each target, we assume 300 instances each of the stateful
and stateless atoms. We spread the 300 instances over 30 pipeline stages; so, a
single stage has 10 instances each of the stateful and stateless atoms.
Relative to a 200 \si{\milli\metre\squared} chip\footnote{This is the smallest
chip size quoted by Gibb et al.~\cite{gibb_parsing}}, the aggregate chip area
overheads (Table~\ref{tab:templates}) for 300 instances of each atom are
modest: it is 0.2 \% for the stateless atoms and 0.9 \% for the Pairs atom, the
most complex among our stateful atoms.

\subsection{Compiling \pktlanguage programs to \absmachine machines}
\label{ss:compiler}
We now consider every target from Table~\ref{tab:templates}, and every
data-plane algorithm from Table~\ref{tab:algos} to determine if the algorithm
can run at line rate on a particular \absmachine machine. We say an algorithm
can run at line rate on a \absmachine machine if every codelet within the
data-plane algorithm can be mapped (\S\ref{ss:code_gen}) to either the stateful
or stateless atom provided by the \absmachine machine. Because stateful atoms
are arranged in a containment hierarchy, we list the \textit{least expressive}
stateful atom/target required for each data-plane algorithm in
Table~\ref{tab:algos}.

\subsection{Lessons learned}
We now reflect on some lessons learned from these evaluations.

\paragraph{Atoms with a single state variable support many algorithms}

For instance, the algorithms from Bloom Filter through DNS TTL Change Tracking
in Table~\ref{tab:algos} can be run at line rate using the Nested Ifs atom that
manipulates a single state variable.

\paragraph{But, some algorithms modify pairs of state variables}

One example is CONGA, whose code we reproduce below:
\begin{verbatim}
  if (p.util < best_path_util[p.src]) {
    best_path_util[p.src] = p.util;
    best_path[p.src] = p.path_id;
  } else if (p.path_id == best_path[p.src]) {
    best_path_util[p.src] = p.util;
  }
\end{verbatim}
Here, \texttt{best\_path} (the path id of the best path for a particular
destination) is updated conditioned on \texttt{best\_path\_util} (the
utilization of the best path to that destination)\footnote{{\tt p.src} is the
  address of the host originating this message, and hence the destination for
the host receiving it and executing CONGA.} and vice versa. These two state
variables cannot be separated into different stages and still guarantee a
packet transaction's semantics. The Pairs atom, where the update to a state
variable is conditioned on a predicate of a pair of state variables, allows us
to run CONGA at line rate.

\paragraph{There will always be algorithms that cannot sustain line rate}

While the targets and their atoms in Table~\ref{tab:templates} are sufficient
for several data-plane algorithms, there are algorithms that they can't run at
line rate.  An example is CoDel, which cannot be implemented because it
requires a square root operation that isn't provided by any of our targets. One
possibility is a look-up table abstraction that allows us to approximate such
mathematical functions. However, regardless of what set of atoms we design for
a particular line rate target (say 1 GHz), there will always be algorithms that
cannot be run at this rate. This is because the set of computations that can be
expressed by an atom (or a finite pipeline of atoms) are finite because these
computations all need to finish within a ns, while the set of algorithms is
infinite.

\paragraph{Atom design is constrained by timing, not area}

Atoms are constrained by two factors, their area and their timing, i.e., the
end-to-end delay on the critical path of the atom's combinational circuit. For
the few hundred atoms that we require, atom area is insignificant (< 1\%)
relative to chip area. Further, even for future atoms that are more
complicated, area is easily controlled by provisioning fewer atom instances.

However, atom timing is more critical. Table~\ref{tab:templates} shows a 3.4x
range in timing between the simplest and the most complex atoms. This increase
in line rate can be explained by looking at the simplified circuit diagrams for
the first three atoms (Table~\ref{tab:circuits}), which show an increase in
circuit depth with atom complexity. The clock frequency of a circuit is at
least as small as the inverse of this timing; consequently, a more complex atom
results in a lower clock frequency and a lower line rate. While all our atoms
have an end-to-end delay under a ns (and hence can run at 1 GHz), it is easy to
extend them with progressively more functionality that causes them to violate
timing at 1 GHz.
%TODO: Try and synthesize this atom.

\begin{table}[!t]
  \begin{scriptsize}
    \begin{tabular}{|p{0.08\textwidth}|p{0.28\textwidth}|p{0.05\textwidth}|}
  \hline
  Atom & Circuit & Min. delay in picoseconds \\
  \hline
  Write & \includegraphics[width=0.2\textwidth]{rw.pdf} & 176 \\
  \hline
  ReadAddWrite (RAW) & \includegraphics[width=0.2\textwidth]{raw.pdf} & 316\\
  \hline
  \pbox{0.1\textwidth}
  {Predicated\\
  ReadAddWrite (PRAW)} & \includegraphics[width=0.3\textwidth]{pred_raw.pdf}  & 393 \\
  \hline
  \end{tabular}
\end{scriptsize}
\caption{Minimum delay of an atom increases with circuit depth. MUX
stands for a multiplexer, RELOP stands for a relational operation between two
operands.}
\label{tab:circuits}
\end{table}


% TODO: Be consistent between compiler target and programmable router.
\paragraph{Compilers can be used to design instruction sets}
Designing an instruction set for a programmable substrate is a chicken-and-egg
problem: the choice of instructions determines which algorithms can execute on
that target, while the choice of algorithms dictates what instructions are
required in the target. Indeed, most other programmable substrates (GPUs, CPUs,
DSPs) go through an iterative process to design a good instruction set.

% TODO: Add a figure for this.
 A compiler can help with this iterative process. To show how, we outline our
own process for designing the atoms in Table~\ref{tab:templates}. We pick a
data-plane algorithm, partially execute our compiler to generate a pipeline of
codelets, inspect the stateful ones, and create an atom that expresses the
computations required across all stateful codelets. We check that an atom can
express the computations we require by fully executing our compiler on the
data-plane algorithm. We then move on to the next algorithm, extending our atom
through a process of trial-and-error to capture more computations---in the
process generating a hierarchy of atoms, each of which works for a subset of
algorithms.

Our atom design process is manual and ad-hoc at this point, but it already
shows how compilers can aid in instruction-set design for programmable
switches. That said, We make no claim to this being the correct or optimal
instruction set. We only show that this is feasible and can be used to
implement a variety of data-plane algorithms, which is far beyond a
fixed-function switch today. We anticipate \absmachine machines evolving as
data-plane algorithms demand more of the hardware.

%%\textbf{Compilation time:}
%%Compilation time is dominated by SKETCH's search procedure.  To speed up the
%%search, we limit SKETCH to search for constants (e.g., for addition) of size up
%%to 5 bits, given that the constants seen within stateful codelets in our
%%algorithms are small. Our longest compilation time is 10 seconds when CoDel
%%doesn't map to a \absmachine machine with the Pairs atom because SKETCH has to
%%rule out every configuration in its search space.  This time will increase if
%%we increase the bit width of constants that SKETCH has to search; however,
%%because the data-plane algorithms themselves are small, we don't expect
%%compilation times to be a concern.
